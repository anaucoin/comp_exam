\documentclass[12pt]{article}
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm
\textheight 21cm


\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multimedia}
\usepackage{graphicx}
\usepackage{media9}
\usepackage{color}
\usepackage{enumitem}
\usepackage[backend=bibtex]{biblatex}
%\bibliography{rtgf19}{}

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}

\title{Establishing consistency and robustness of neural networks for large data sets}
\author{Alexa Aucoin$^{1}$ \\ \normalsize{$^{1}$Department of Mathematics, University of Arizona}}
\date{\today}


%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document}
\maketitle

\hrule
\begin{sciabstract}
\begin{center} \large{Abstract}\end{center}
Developing tools to make neural networks robust and consistent across a wide range of data is of significant interest to data scientists. This comprehensive exam paper presents detailed examples of neural networks which suffer from data inconsistency or fragiility and discusses some possible avenues to recitfying these short-comings. In particular, we first look at a large set of primate brain activity data and highlight how simple classification networks fail to produce consistent results across the entire data set. We then present possible techniques and architectures thoughtfully chosen to improve the generalizability of a network trained using this data. We then shift our focus to the problem of robustness by exploring residual networks subject to adversarial attacks. Drawing from a dynamical systems perspective, many have used the addition of noise to increase the robustness of a trained network. This chosen noise is often small but arbitrary and, in large systems, can increase the complexity of a network significantly. We discuss a potential choice of noise which is informed by the training data set and perhaps simpler to implement. This work is ongoing and future directions are also discussed at the end of the paper.
\end{sciabstract}
\hrule

\section{Introduction}Neural networks (NNs) have become increasingly popular as a tool to discover key patterns and correlates in data. They have been shown to be effective in a variety of classification and regression tasks, however, how these complex sytems work remains opaque. Training these networks often requires large data sets and, once trained, the network itself can be quite fragile. Developing tools to make NNs robust and consistent across a wide range of data is therefore of significant interest to data scientists. \\
\indent Large NNs are prone to overfitting and in practice often generalize poorly on unseen data. Much work has been done to study to trade-off between network complexity and generalizability including \textcolor{red}{REFERENCES}. These techniques provide empirical evidence that deep neural networks (DNNs) are able to generalize but realizing such successful generalizability is heavily dependent on the choice of network design. These choices include, but are not limited to choices of input regularization, network architecture, loss functions, and optimization algorithms. In some instances, significant variability in the representations within a dataset can also contribute to poor generalization. In Section 3, we present an example of such a variable data set and discuss possible techniques and network designs to improve network generalizability. \\
\indent In addition to issues of generalizability, trained neural networks also remain vulnerable to adversarial attacks. This is a significant concern in networks designed tolearn  security- or safety-critical tasks. Over the last decade, a lot of work has been done in an attempt to make neural networks more robust to pertubations. \textcolor{red}{REFERENCES} In Section 4, we present a framework for choosing additive noise as a means to bolster robustness. In the Section 5 we discuss future directions of this work. In particular, we present the potential for connecting these two projects to inform the design of more generalizable and robust neural networks.

\section{Neural Networks}
\indent Artificial neural networks are a collection of processing nodes (neurons) that are connected to one another through weights. The neurons are so named because they loosely mimic the computations done by the human brain. Each neuron accepts a weighted sum of inputs which is added to the neuron's bias. The bias can be thought of as the analog to the threshold value of a biological neuron. This sum, of the weighted input and bias, is then passed through an activation function which determines if and to what strength the neuron outputs information about the input signal to subsequent neurons. Mathematically, the computation done by the $i$-th neuron can be represented as
\begin{equation}
  \hat y_i = a_i(\sum_{j=1}^N w_{ij}x_j + b_i)
  \label{eq:unit}
\end{equation}
where $a$ is the chosen activation function, $N$ is the number of input units, and $x,\hat y,w,$ and $b$ are the input, output, weights, and bias respectively. These neurons are often organized into layers, starting with the input layer, followed by any number of hidden layers, and ending with an output layer. A hidden layer consists of hidden neurons which are defined by their choice of linear or non-linear activation function $a$.  \\
\indent There are many choices of network design which determine how the input propagates through the network. Feed-forward networks are networks which allow information to propogate only from one layer to the next, while recurrent neural networks (RNNs) also allow for feedback loops. These loops can be a backwards connection from a layer to itself or any number of the preceeding layers. Additionally, two adjacent layers may be fully connected, where each neuron in a single layer accepts input from each neuron in the preceeding layer, or pooled, where each neuron only accepts input from a subset of neurons in the preceeding layer. Choices related to network design are often informed by the type of input data and the desired learning task. \\
\indent The goal of these artificial networks is to approximate a function $f$ which maps a set of input data $x$ to an output $y$. This mapping is defined by the set of parameters, $\theta$, which includes all of the weights and biases in the network. The process of updating the parameters $\theta$ to more accurately (defined with respect to a chosen loss function) map input data to an output is called learning. For the purposes of this paper, we will consider only supervised learning tasks, but interested readers may refer to \cite{bengio} for details on unsupervised learning algorithms. In supervised learning, each observation in the training set, $x$, is associated (or labelled) with a desired output, $y$, called a target. The network learns to approximate the mapping $f$ by examining sample observations of the desired mapping. We call this collection of pairs $(x,y)$ the training data. \\
\indent During a supervised learning task, an example from the training set is given as input to the network. The sample observation $x$ is propogated through the network and outputs an approximation $\hat y$. The accuracy of the network is then given by the distance between the approximated target $\hat y$ (outputted by the network) and the true target $y$. This distance is determined by a loss function, which is chosen during the design of the network. As with network design, choice of loss function will also be influenced by the desired learning task. For example, (binary) cross entropy, is often used for classification tasks while mean squared error (MSE) is commonly chosen for regression tasks. \\
\indent The process of updating the weights $\theta$ to better approximate $f$ is done through an optimization algorithm which drives the parameters towards a local minimum of the cost function. Algorithms such as gradient descent and stochastic gradient descent are often used for the choice of optimizer. During opitimization, the gradient of the loss funcion with respect to $\theta$ is computed by a process called backpropogation. Readers can refer to \cite{bengio} and many other sources for details on how backpropagation simply efficiently compute the gradient.  \\
\indent Learning is said to be complete when presenting the network with more observations from the training data does not signiciantly improve network accuracy. When learning is complete, the network parameters are fixed and can now be used to predict output values for new observations outside of the training set. Due to the large number of network parameters, it is possible for the network to experience over-fitting. This means the network performs very well for observations within the training data set, but exhibits poor accuracy for new data. Overfit networks suffer from a lack of generalizability. To mitigate issues of generalizability, it is common to set aside a set of validation data. This is data that is held separate from the training set and periodically passed through the network to check validate accuracy on new data.  \\
\indent There are many validation strategies used during training that can help mitigate over-fitting. However,it no is not the only aspect of NN training that can lead to a lack of generalizability. Variability in the representations of the training data can also limit a network's generalizability. Careful choices of data processing, network architecture, and training are therefore an essential part of designing a network that is useful on never-before-seen data. However, the analysis of NNs remains a black-box mystery and guidance on the ``best" network design remains context dependent and highly reliant on experienced intuition. In the following section, we present a concrete example of an experimental data set with significant variability in the representations along with some strategies to design reliably accurate NNs for this training set.
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Classic SVM visual(?).}
% \end{figure}
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Example of a standard feed-forward network architecture.}
% \end{figure}
%\section*{Generalizability}
\section{Experimental data}To highlight how variability in training set representations poses problems with reliable network design, we consider a large set of neural recording data. The data was collected from a primate experiment conducted by Katalin Gothard in the Gothard Lab at University
of Arizona \cite{lab}. The experiment is broken up into five time blocks: 3 blocks of machine delivered puffs and 2 blocks of human touch. For this paper, we are particularly interested in the block of neural data recorded during the human touch stimulus. During a touch block, a human handler (with whom the monkey has a positive repore) touches the monkeys face in a sweeping gesture 10 times across the left upper muzzle (LUM), and then 10 times accross the left brow (LBR). This grouping of touches, reffered to as a touch trial, is repeated 4 times in each touch block for a total of 80 touches. The duration of the touch is approximately 1 second long, with 4 seconds between each touch.  \\
\indent The two brain regions recorded during these experiments are 3b and the amygdala. 3b is a portion of the primary somatosensory cortex responsible for decoding sensory inputs related to touch. This region on the brain is well studied and convenient because the neurons in 3b have a very structured receptive field. One can think of a neuron's receptive field as the portion of the input space that the neuron will respond to. In other words, it is the group of inputs that drive that neuron's spiking activity. The receptive field in 3b is so nicely structured that one can create a map of the face where each region corresponds to a group of neurons in 3b. This is a particularly nice feature as it makes designing experiments and analyzing data much more straightforward. \\
\indent The amygdala is the part of the brain that is responsible for processing our
emotions. Almost all information is passed through the amygdala at some point or
another. For example, information on motor functions need to be sent to the amygdala
so it can anticipate changes in sensory stimulus. The amygdala is
constantly receiving and decoding information about our surrounding environment from all brain areas in
order to inform our emotional responses. Because of its complex input, the amygdala is generally not well understood or even often studied. Unlike 3b, the amygdala does not have a well-structured receptive field.  By working with the data collected at the Gothard lab,  we have a unique opportunity to study and learn from spiking activity in the amygdala. \\
\indent During the experiment a probe is placed within the monkey's brain and records electric pulses transmitted by the neurons in 3b and the amygdala. These electrical pulses are referred to as spikes and are thought to contain important information about changes in the environment that may impact the body.  Through a complex inverse problem, the electric pulses recorded by the probe are is assigned to a particular
neuron. The methodology for this inverse problem is beyond the scope of this paper,
but an interested reader can refer to the associated master's thesis \cite{thesis}. Once all spike
times are associated with a particular neuron, we obtain the brain activity of the primate
as a series of spike times, referred to as spike trains, for each recorded neuron.  \\
\indent During the experiment, the monkey's heartrate is also recorded. Changes in the primate heartrate can be used as in indicator that the monkey undergoing a change in emotional state due to the presence of a stimulus. This is particularly useful as we expect that the human touch stimulus is an enjoyable one for the primate. A significant finding of this experiment was the drop in the primate's baseline heartrate during a touch trial \ref{katipaper}. It is reasonable then, to expect there exists a correlation between the recorded spiking activity and the underlying presence (or lack) of stimulus. This provides resonable evidence that a sufficiently designed neural network could learn to infer the underlying stimulus from spatiotemporal spiking data. Additionally, machine learning techniques have recently been successful in analyzing/modelling experimental spike trains obtained from more well-studied regions like V1, hippocampus, olfactory systems \cite{laza} \cite{banino} \cite{stevens}. There is reason to believe it may also be useful in understanding how information is encoded or decoded in 3b and even more complex regions like the amygdala. In particular, we would like to train a simple classification network to detect the difference between spike activity that is recorded during the presence of a touch stimulus versus latent (baseline) spike activity. \\
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Visual of experimental set-up}
% \end{figure}
\subsection{Objectives and obstacles}
\indent On any given day of experimental data, we have $N$ ordered lists of spike times, where $N$ is the number of individual 3B cells recorded. To prepare the experimental data to be fed through a feed-forward network, we first discretize the time of the experiment into blocks of 5ms. For each time block of 5ms, and for each $N$ we count the number of spikes occuring in that 5ms window. This yields a vector of spike counts of length $N$. We then append 5 of these vectors together, which yields a matrix of spike counts of length $5N$  corresponding to a 25ms window in the experiment. We do this for the duration of the experiment, and assign each matrix one of two labels: stimulus or latent. Matrices labelled as stimulus correspond to spiking activity during 25ms of active touch stimulus, while matrices lablled as latent correspond to spiking activity which occured entirely during a period of latency (no stimulus present). These latent times can occur between touches or between experimental blocks. Since these periods of latent were longer and more frequent, there are much more latent data points than there are stimulus data points. To ensure that both labelled classes were represented equally, we randomly sample from the latent data with replacement until we had an equal number of data points in both classes. The labelled matrice of latent and stimulus data make up our training data set. \\
\indent The variability in the data comes from pooling together data recorded on different days of the experiment. Between different days of experimental recordings, the probe is left in the primate brain and it is very common for the probe to shift during this time. As such, we can expect that the spiking activity is recorded from a new population of neurons for each day of the experiment. Additionally, due to the senstivity of the patches on the probe, the number of neurons recorded on any given day is also not guarenteed. This creates a significant problem when trying to use the spike data across days as input into the same neural network. Certainly we can expect that the function mapping the spike data to an underlying stimulus remains similar across days, but on any given day we have significant variability in the number of cells recorded. In the remainder of this section, we attempt to address the following questions: How do we design a network to accept variable-size input data while being reasonably confident it will generalize to new data from the same experiment? Can we design such a network that is reasonably complex without over-manipulation of the data?

\subsection{Preliminary Results}
\indent Before we tackle the variability across days, we first test the performance of simple neural networks on data from a single day of the primate experiment. This is a useful task for exploring the kinds of network design choices that result in an a efficient, accurate neural network for this kind of experimental data. We start first with two simple supervised learning algorithms:  logistic linear regression and support vector machine (SVM) with a linear kernel. These linear algorithms are very easy to implement and their training is well-studied. As such they are a natural starting point before applying more complex learning algorithms. For both algorithms we tested a range of values for $\lambda$, the regularization strength. This is a tunable hyperparameter that helps to ensure generalizability of data by encouraging network sparsity. Larger $\lambda$ leads to stronger penalties for large parameter values in $\theta$. Table \ref{tab:simp} summarizes the best validation accuracy achieved for each algorithm. \\

\begin{table}[]
  \centering
\begin{tabular}{|c|c|c|}
  \hline
                Algorithm    & Accuracy & $\lambda$  \\
                      \hline
Logistic Regression & 71.63\%  & 0.007   \\
  \hline
SVM                 & 71.14\%  & 0.025    \\
  \hline

\end{tabular}
\caption{Results of two simple supervised learning algorithms for a single days of touch data where $N = 49$. Validation accuracy shown is the best achieved out of range of regularization strengths  $\lambda \in [10^{-6},10^{-0.5}]$.}
\label{tab:simp}
\end{table}

These simple linear algorithms perform both perform similarly, and certainly leave much room for improvement. This suggests our data may not be linearly separable. To introduce some nonlinearity and model complexity, we trained a three-layer long-short term memory (LSTM) network. Details about the architecture of LSTMs can be found in \cite{bengio}, but the important feature of the LSTM network is the ability to identify order dependencies in a sequence by storing information about past inputs. This allows for more context based classification decisions which is potentially relevant in the context of neural recordings. The shallow LSTM is also implemented fairly easily. \\
\indent To avoid over-fitting during training of the LSTMs, we implemented two standard techniques: dropout and bootstrapping. Dropout is a regularization method in which neurons in the network are randomly excluded from the network during training. This is achieved by setting their activations and weights to 0. This introduces sparsity into the network and potentially prevents over-fitting. Bootstrapping does not introduce network sparsity, but rather increases the size of the training set by sampling the training data with replacement. Empirical evidence shows that providing the network with more samples from the input distribution allows for more reliable estimators about the distribution. Thus, bootstrapping is useful in the potential prevention of over-fitting. Table \ref{tab:LSTM} shows the results for the LSTM training exercise. Although the plain LSTM and dropout LSTM algorithms preformed worse than the linear classification algorithms, the significant victory here is the validation accuracy achieved on single day with bootstrapping.

\begin{table}[]
  \centering
\begin{tabular}{|c|c|c|}
  \hline
                Algorithm    & Accuracy   \\
                      \hline
LSTM (plain) & 68.42\%   \\
  \hline
Dropout LSTM                 & 64.98\%     \\
  \hline
Bootstrap LSTM                 & 95.22\%     \\
\hline
\end{tabular}
\caption{Validation accuracy of three-layer LSTM trained on a single days of touch data where $N = 49$ with no regularization techniques, dropout and bootstrapping.}
\label{tab:LSTM}
\end{table}


% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{SVM single day.}
% \end{figure}
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Training and validation accuracy graphs. W/ and w/o boot strapping(?)}
% \end{figure}
The results of this groundwork provides evidence that relatively straight-forward neural networks can successfully learn on data from a single day of the experiment.  Recall that our aim is to design a network that can be reliably extended to classify experimental data from any given day. This necessarily means addressing the issue of non-uniform input size across days.
\subsection{Methods and potential algorithms}
One possible way ensure our algorithm is accruate across days is to be able to train one network on a training set consisting of data from across all days of the experiemnt. This is difficult with the data we currently have since the shape of the input space changes across days. One way to rectify this is to project each days data onto a subspace of prescribed size $k$. In our experimental case, this can be thought of as picking a $k$ dimensional subset of the $N$ recorded neurons. This can consist of choosing $k$ indivual neurons or $k$ linear combinations of neurons. Applying this dimensional reduction for each day results in a uniform size of input data across days. Of course, there is always a trade-off between dimensional simplicity and accuracy. In projecting onto this lower dimensional subspace, we are losing potentially important information. Principal component analysis or PCA is a natural way to project data onto a lower dimensional subspace while also preserving the features of the data with the most variability. This is particularly desirable in applications to machine learning where variability is seen as containing crucial information. PCA can potential transform non-separable data into separable data. As we have seen, the separability in data points allows for greater success in classification tasks.\\
We start by computing the PCA for each of the twelve days' data. To compute PCA for a given day, we must first standardize the statistics for each neuron. This is necessary since PCA is particularly sensitive to variance across input features. In this case, the input features are the spike counts for a particular neuron in a single time bin. Thus, there is a total of $5N$ input features.  For ease of computation, we  flatten each $N\times5$ input matrix to be a single vector of length $5N$.  We then compute the standardized data $\tilde{x}$ by
$$ \tilde{x_i} = x_i - \mu_i / \sigma_i $$
where $\mu_i$ and $\sigma_i$ are the mean and standard deviation for the $i$-th input feature respectively. Once standardized, we compute the covariance matrix for $\tilde{x}$ and its eigenvector and eigenvalues. The eigenvectors of the convariance matrix are the principal components, the direction along which the data has the most variability. Their associated eigenvalues can be thought of as a coefficient which quanitfies the amount of variability in that direction. By dropping the last $N-k$ principal components and projecting the data along the remaining subspace, we can transform the data across days while losing the least amount of variability. This allows us to have a uniform input size across days. \\
Below is a visualization of the PCA computed for all twelve days. Using the PCA is a simple and quick way to work around the issue of input space variability but it leaves something to be desired. We see that across days the PCA is qualitatively similar. There is no significant decrease in the magnitude of the eigenvectors making the choice of the lower dimension $k$ unclear. Additionally, the point clouds for the first few components (where the data has the most significant variability), are very non-separable. This is of significant importance when we choose a classification network architecture since it is likely that a simple feed-forward network of linear layers will not have much success. \\
% \includegraphics[width=5cm]{example-image-a}
% \caption{PCA point cloud shows not good separability linearly}
% \end{figure}
By using PCA to transform the existing experiemental data, we can create a training set of uniform input size that represents multiple days of experimental data. This allows us the freedom to implement standard network designs. PCA also equips us with the means to transform completely new data into a space consistent with our current training set. This is important as we want to be able to handle experimental data with an input size not previously seen before. \\
\indent There are some concerns with using PCA as a way to preprocess the training data that are worth discussing. As mentioned above, there is the concern with  dimensional simplicity at the expense of accuracy. With no obvious threshold cut-off for principal component variability and the significant difference in the number of components across days, there is a legitimate concern that the PCA may be throwing away too much information embedded within the data. Additionally, the PCA must be computed on each days data individually. This means there is really guarentee that the $k$-dimensional projections of data from different days are encoded with the same feature information.\\
 \indent It is also important to mention that there is little to no biological or real-world meaning that can be interpreted by the principal components. Using PCA largely means giving up the hope for a tractable and practical understanding of any classification results obtained from this data. If the long term goal is to use machine learning to inform our hypthosis about the computational mechanisms in the brain, PCA is not necessarily the top choice. Nevertheless, the computation is quick and simple to implement and classification algorithms using the PCA data are worth exploring. \\
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Results of multiday data through network similar to single day network (need to compute this)}
% \end{figure}
\indent An alternative way to make the input shape uniform across days comes from an idea inspired by Generative Adversarial Networks (GANs). A GAN consists of two competeing sub-networks, a Generator and a Discriminator, and is best understood by a more colloquial analogy. Consider the problem of a banker and a criminal trying to create a counterfeit dollar bill. In this analogy, the buyer plays the role of the Discriminator network and the criminal plays the role of the Generator network. The criminal will first generate some counterfeit money and try to deposit the it at the local bank. The banker will examine the bill counterfeit against real bills, point out the discrepancies, and send the criminal home. The criminal, now equipped with the knowledge of what went wrong, tries to improve his counterfeit bill and try again. This will continue until the criminal makes a counterfeit bill so good that the discriminator cannot tell the different between the fake and a real bill. Training for a GAN network follows the following steps:  \\
\textbf{Generator:} \\
\begin{enumerate}
  \item Create samples of latent space data using random noise.
  \item Forward propagate noisy data through the generator to create ``fake" data.
  \item Pass the generated data through the discriminator and get predictions.
  \item Calculate the discriminator's output loss on the generated data with the data falsely labelled as ``real". (This step is crucial as we want an error in the discriminators classification to \textit{positively} impact the weights of the generator.)
  \item Backpropagate through the generator \textbf{only}.
  \end{enumerate}
  \textbf{Discriminator:} \\
  \begin{enumerate}
    \item Pass a batch of real data to the discriminator.
    \item Pass a batch of generated data to the discriminator.
    \item Average the loss from Steps 1 \& 2.
    \item Backpropagate through the discriminator only.
  \end{enumerate}

\indent The key concept in GANs is that the competing networks work together to learn the key features of the input data. Error from the discrimintors loss is back-propogated through the network and used to update the weights of the generator. By competing against the discrimintor, the generator learns how to map a sample from one distribution space (often a random distribution) to something that mimics, quite accurately, a sample from the desired distribution space. We propose a modified GAN framework to rectify the dimensional variability in our experimental data. \\
\indent Our modified GAN framework will be made up of three types of networks: an encoder, a decoder, and a discriminator. It will often be useful to consider a specific encoder and decoder together as one pair. We refer to this pairing as an autoencoder. In this framework, the autoencoder plays the role of the generator. Suppose we train a separate autoencoder for each day of experimental data. The first part of the autoencoder, the encoder, will accept as input a vector of spike counts of length $N\times 5$ and ouput an encoded representation of the data of specified length $k$. The second part of the autoencoder, the decoder, will accept as input the encoded $k$-dimensional representation of the data and output a reconstructed version of the original $N\times 5$-length vector of spike counts. The third part of the network is a single discriminator, this discriminator will accept as input the $k$-dimensional encoded data and produce an output which classifies what day the representation comes from. \\
\indent The training of our modified GAN network will work as follows:
\textbf{Initial start-up:}
\begin{enumerate}
  \item Train an autoencoder to encode and decode data from Day 1 of the experiment. Training will stop when the autoencoder can accurate reconstruct the original data from the $k$-dimensional encoded representation.
\end{enumerate}
\textbf{Generator}
\begin{enumerate}
  \item Initialize a new autoencoder.
  \item Generate $k$-dimensional encoded data.
  \item Pass the $k$-dimensional encoded data through the discriminator and get predictions.
  \item Calculate the discriminator's output loss on the new day's encoded data with the data falsely labelled as ``old". (This step is crucial as we want an error in the discriminators classification to \textit{positively} impact the weights of encoder.)
  \item Forward propagate the encoded data through the corresponding decoder.
  \item Calculate the reconstruction loss.
  \item Take a convex combination of the discrimination loss and reconstruction loss.
  \item Backpropagate through the autoencoder.
  \end{enumerate}
  \textbf{Discriminator:} \\
  \begin{enumerate}
    \item Pass a batch of old days' data to the discriminator.
    \item Pass a batch of new day's data to the discriminator.
    \item Average the loss from Steps 1 \& 2.
    \item Backpropagate through the discriminator only.
  \end{enumerate}
  \textbf{Repeat Generator and Discriminator steps for each additional day of data.}
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{GAN architecture visual}
% \end{figure}
\indent Using this framework, we design a network to learn the best $k$-dimensional representation of the data rather than force our choice of low dimensional representation. Using the same discriminator across days ensures that the $k$-dimensional representations are all encoded in a similar way, while the individual decoders enforce that not too much data is lost by the dimensional reduction. This is a potentially useful way to make the input dimensions consistent across all experimental days as it overcomes two of the main concerns of the PCA approach. Intrepetability of the $k$-dimensional representation using the GAN approach is still unclear, as it relies on the analysis of a group of trained networks, which is still a topic of interest that is not well understood. \\
The concern with using this approach is mainly due to the fact that it is fairly complex. It requires training a new network for each new day of data, which can potentially be time consuming. Though there are some techniques in transfer learning may help ameliorate this. Additionally, it is unclear whether the overall accuracy of all data encoders will converge to a shared minimum. Early implementations of this network architecture have been successful in creating encoded data for the first 5 days or so, but additional days lead to a overall blowup of training accuracy. Work to remedy this issue and develop a stable training algorithm is still on going. \\
\indent In the next section, we shift our focus from possible approaches to improve generalizability to ones aimed at improving robustness of trained neural networks. Both are important obstacles of interest in the application of machine learning to large data sets which we hope to address in future work.
\section*{Robustness}
\section{Residual Networks}
Residual networks (ResNets) were first introduced in 2015 to address the issue of exploding/vanishing gradients and have sinced made training ultra-deep networks (networks over 1000 layers) pragmatic. In previous discussion, we considered a mapping, $y = f(x,\theta)$ from input data $x$ to target $y$ with parameters $\theta$, to be learned by a feed-forward network. Assuming that the input and output have the same dimension, we can ask that the network instead learn the residual mapping $r(x) = f(x)-x$. Thus, the true mapping will be $F(x) = r(x) + x$. While the network should be able to approximate both functions asymptotically in theory, the residual network formulation allows for a better conditioned learning task. \\
\indent The problem of exploding/vanishing gradients is called the degradation problem. As more layers are added to neural networks, the network will begin to converge, then become saturated, and then abrubtly loses accuracy with the addition of more layers. This degradation of accuracy suggests that, in the presence of many nonlinear layers, the network may have difficulty in approximating near identity mappings. \\
\indent In the residual learning framework, we assume that mapping between input and output is near-identity. In the case where the identity mapping is optimal, the network can drive the weights towards zero to approximate an identity mapping. In practical cases,the identity mappings will likely not be the optimal mapping, but nevertheless the residual learning framework has been shown to precondition the learning problem better, with learned residual functions having much smaller responses in general \cite{res}. Ensembles of residual networks, which consist of a group of residual networks trained simultatneously, have been shown to be extremely accurate on practical image recognition tasks.
\subsection{Adversarial attacks}
Although the residual learning framework made training DNNs pragmatic, these deep networks remain vulnerable to adversarial attacks and there is a reluctance to apply DNNs to practical applications where security and/or safety is crucial. This includes but is not limited to automonomous vehicles, robotics, malware detection etc.. \textcolor{red}{REFERENCES}
\subsection{Objectives}
\section{A proposed model}
\subsection{ResNets as neural ODEs}
We consider a residual network that is made up of a series of pre-activated residual blocks. Each block can be thought of as a layer that computes a residual mapping. The $l$-th residual block take in input $x_l$ and computes the mapping
$$ x_{l+1} = x_{l} + F(x_l,\theta_l)$$
where $x_0$ is a sample from the training set $T \subset \mathbb{R}^d $ and the parameters $\theta_l$ are learned through back-propogation of the loss function. Here $F(x_l,\theta_l)$ is a pre-activation residual block and can be written as
$$ F(x_l,\theta_l) =  \theta_l^{C2}\otimes\sigma(\theta_l^{BN2}\odot\theta_l^{C1}\otimes\sigma(\theta_l^{BN1}\odot x_l))$$
where superscript BN are batch normalization layers, supercript C are convolutional layers and $\odot, \otimes$ represent that batch normalization and convolution operators respectively. The activation function $\sigma$ is an application of the rectified linear unit (ReLU). ReLU is the linear piecewise function
\begin{equation*}
\sigma(x) = \begin{cases} x & x > 0 \\
 0 & x \leq 0. \\
\end{cases}
\end{equation*}
In a supervised learning task, each element $x \in T$ will have an associated label $y$. We can represent the full ResNet as

\begin{equation} \begin{cases}
x_{l+1} = x_l + F(x_l,\theta_l), & l=0,\dots,L-1 \\
x_0 = x \\
\hat y = f(x_L)
\end{cases}
\label{eq:sys}
\end{equation}

where L is the number of residual blocks, $f(x) = \text{softmax}(\theta \cdot x)$ and $\hat y$ is the network's prediction of true target label $y$ for the initial input $x$. \\
\indent It is helpful to view the training of the ResNet as a an ODE. To do so, we will introduce a temporal discretization where ``time" refers to the layer depth. Let $\Delta t_l = l/L$ for $l=0,1,\dots,L$ and $x_l = x(t_l)$, then Eq. \ref{eq:sys} becomes
\begin{equation} \begin{cases}
x(t_{l+1}) = x(t_l) + F(x(t_l),\theta(t_l)), & l=0,\dots,L-1 \\
x(0) = x \\
\hat y = f(x(1)).
\end{cases} \end{equation}
Multiplying the $F(x(t_l),\theta(t_l)$ term by $\Delta t/\Delta t$ and defining $\tilde{F} = \frac{1}{\Delta t} F$ we obtain the forward Euler discretization
\begin{equation}
  \dfrac{dx(t)}{dt} = \tilde{F}(x(t),w(t)),\text{   }  x(0) = x.
\end{equation}
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{ResNet architecture visual}
% \end{figure}
% \subsection{Dynamical systems and noise}
% From the transport equation to neural network... How added noise changes the decision boundaries \\
% \begin{figure}[h!]
% \center
% \includegraphics[width=5cm]{example-image-a}
% \caption{Changes to boundaries as noise is increased for tranpsort eq. (myself or can I pull this from UCLA paper??)}
% \end{figure}
\subsection{ResNet with added noise}
Performance with added noise.
\subsection{How much noise?}
Rationale for a simplified choice of noise that is informed by the data...\\
NEED TO DISCUSS THIS(?)
\section{Discussion and Future Work}
Plan for dissertation research...

%\pagebreak
%\printbibliography

\end{document}
